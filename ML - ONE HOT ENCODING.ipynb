{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding is used to convert categorical variables into a numeric format so that it can be used by the machine learning algorithms. The basic idea is to create variables that will take up the values 1,0 to represent each categorical value. We do this,as ML algos do not understand text,but only numbers.<a href=\"#One-hot-encoding-is-used-to-convert-categorical-variables-into-a-numeric-format-so-that-it-can-be-used-by-the-machine-learning-algorithms.--The-basic-idea-is-to-create-variables-that-will-take-up-the-values-1,0-to-represent-each-categorical-value.-We-do-this,as-ML-algos-do-not-understand-text,but-only-numbers.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # here we are creating our own dataset \n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    # creating our demo data\n",
    "    # here we are importing the pandas library for creating the demo data \n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    # creating our dataframe\n",
    "    # here we have created the variable called data \n",
    "    # then by using the pandas library we have created the dataframe\n",
    "    # we have created 2 lists in the dictionary \n",
    "    # and then we have printed the data \n",
    "\n",
    "    data = pd.DataFrame({'Name':['anna','anna','bob','cynthia','cynthia','bob','anna'],\n",
    "                        'Marks':[56,65,78,99,54,34,88]})\n",
    "    print(data)\n",
    "\n",
    "          Name  Marks\n",
    "    0     anna     56\n",
    "    1     anna     65\n",
    "    2      bob     78\n",
    "    3  cynthia     99\n",
    "    4  cynthia     54\n",
    "    5      bob     34\n",
    "    6     anna     88\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    # SO we will use one-hot-encoding to convert the names to a numeric counterpart so that our\n",
    "    # ML algorithm can understand it\n",
    "    # here we are importing the one hot encoder module from the sklearn.preprocessing package \n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    # create an instance/object from the class OneHotEncoder\n",
    "\n",
    "    encoder=OneHotEncoder()\n",
    "\n",
    "    #  here we have the variable caled encoder data \n",
    "    #  in the dataframe we will fit and transform  the name column in the data and display\n",
    "    # it in the form of an array \n",
    "    #  so we are fitting the name column \n",
    "\n",
    "    encoder_data = pd.DataFrame(encoder.fit_transform(data[['Name']]).toarray())\n",
    "\n",
    "    # encoding the name and converting it in a 2D array\n",
    "    #  here the data has been encoded \n",
    "\n",
    "    encoder_data\n",
    "\n",
    "    # 0-is the encoding for the first group 'Anna'\n",
    "    # 1- is the encoding for the second group 'Bob'\n",
    "    # 2 - is the encoding for the third group 'Cynthia'\n",
    "\n",
    "Out\\[6\\]:\n",
    "\n",
    "|     | 0   | 1   | 2   |\n",
    "|-----|-----|-----|-----|\n",
    "| 0   | 1.0 | 0.0 | 0.0 |\n",
    "| 1   | 1.0 | 0.0 | 0.0 |\n",
    "| 2   | 0.0 | 1.0 | 0.0 |\n",
    "| 3   | 0.0 | 0.0 | 1.0 |\n",
    "| 4   | 0.0 | 0.0 | 1.0 |\n",
    "| 5   | 0.0 | 1.0 | 0.0 |\n",
    "| 6   | 1.0 | 0.0 | 0.0 |\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    # merging the encoded columns(encoder_data) to the original dataframe\n",
    "    # here we have created the variable called final data\n",
    "    #  then we have joined or merged the encoder data to the original data \n",
    "\n",
    "    final_data = data.join(encoder_data)\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    # view the final_data to see if the encoding is done 0=anna,1=bob,2=cynthia\n",
    "    #  here we have checked the final data  to checj if the categorical variables have been \n",
    "    # converted to the numerical \n",
    "\n",
    "    final_data\n",
    "\n",
    "Out\\[8\\]:\n",
    "\n",
    "|     | Name    | Marks | 0   | 1   | 2   |\n",
    "|-----|---------|-------|-----|-----|-----|\n",
    "| 0   | anna    | 56    | 1.0 | 0.0 | 0.0 |\n",
    "| 1   | anna    | 65    | 1.0 | 0.0 | 0.0 |\n",
    "| 2   | bob     | 78    | 0.0 | 1.0 | 0.0 |\n",
    "| 3   | cynthia | 99    | 0.0 | 0.0 | 1.0 |\n",
    "| 4   | cynthia | 54    | 0.0 | 0.0 | 1.0 |\n",
    "| 5   | bob     | 34    | 0.0 | 1.0 | 0.0 |\n",
    "| 6   | anna    | 88    | 1.0 | 0.0 | 0.0 |\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    # Drop the original 'Name' variable from our dataframe\n",
    "    #  here from the final data we have drop the original name column  from the column \n",
    "    #  and then we have  printed the final data \n",
    "    # axis=0 means rows and axis=1 means columns\n",
    "    # implace=true makes parmanent changes to the dataframe\n",
    "\n",
    "    final_data.drop('Name',axis=1,inplace=True)   \n",
    "    final_data                                    \n",
    "\n",
    "Out\\[9\\]:\n",
    "\n",
    "|     | Marks | 0   | 1   | 2   |\n",
    "|-----|-------|-----|-----|-----|\n",
    "| 0   | 56    | 1.0 | 0.0 | 0.0 |\n",
    "| 1   | 65    | 1.0 | 0.0 | 0.0 |\n",
    "| 2   | 78    | 0.0 | 1.0 | 0.0 |\n",
    "| 3   | 99    | 0.0 | 0.0 | 1.0 |\n",
    "| 4   | 54    | 0.0 | 0.0 | 1.0 |\n",
    "| 5   | 34    | 0.0 | 1.0 | 0.0 |\n",
    "| 6   | 88    | 1.0 | 0.0 | 0.0 |\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    # here we are renaming the column of the dataframe that is the final data \n",
    "    # then we have printed the final data \n",
    "\n",
    "    final_data.columns =['Marks','Anna','Bob','Cynthia']\n",
    "    final_data\n",
    "\n",
    "Out\\[10\\]:\n",
    "\n",
    "|     | Marks | Anna | Bob | Cynthia |\n",
    "|-----|-------|------|-----|---------|\n",
    "| 0   | 56    | 1.0  | 0.0 | 0.0     |\n",
    "| 1   | 65    | 1.0  | 0.0 | 0.0     |\n",
    "| 2   | 78    | 0.0  | 1.0 | 0.0     |\n",
    "| 3   | 99    | 0.0  | 0.0 | 1.0     |\n",
    "| 4   | 54    | 0.0  | 0.0 | 1.0     |\n",
    "| 5   | 34    | 0.0  | 1.0 | 0.0     |\n",
    "| 6   | 88    | 1.0  | 0.0 | 0.0     |\n",
    "\n",
    "### This is usually used in NLP(NaturalLanguageProcessing) ,and Sentiment Analysis<a href=\"#This-is-usually-used-in-NLP(NaturalLanguageProcessing)-,and-Sentiment-Analysis\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "### Encoding is used to transform categorical variable into numeric features. Label Encoding is an easy technique to encode categorical variables. Howver there can be an issue when nominal encoded variables might end up being interpreted as ordinal.<a href=\"#Encoding-is-used-to-transform-categorical-variable-into-numeric-features.-Label-Encoding-is-an-easy-technique-to-encode-categorical-variables.-Howver-there-can-be-an-issue-when-nominal-encoded-variables-might-end-up-being-interpreted-as-ordinal.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    # techniques used for encoding variables\n",
    "    # here we are importing the libraies to perform the encoding \n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    # importing the label encoder\n",
    "    # here we are importing the preprocessing module from the sklearn package \n",
    "\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    # here we have created the variable called iris dataframe \n",
    "    # then by using the pandas library we are reading the iris csv file \n",
    "    # then we are checking initial 5 rows of the dataframe \n",
    "\n",
    "    iris_df = pd.read_csv('Iris.csv')\n",
    "    iris_df.head()\n",
    "\n",
    "Out\\[11\\]:\n",
    "\n",
    "|     | sepal_length | sepal_width | petal_length | petal_width | species |\n",
    "|-----|--------------|-------------|--------------|-------------|---------|\n",
    "| 0   | 5.1          | 3.5         | 1.4          | 0.2         | setosa  |\n",
    "| 1   | 4.9          | 3.0         | 1.4          | 0.2         | setosa  |\n",
    "| 2   | 4.7          | 3.2         | 1.3          | 0.2         | setosa  |\n",
    "| 3   | 4.6          | 3.1         | 1.5          | 0.2         | setosa  |\n",
    "| 4   | 5.0          | 3.6         | 1.4          | 0.2         | setosa  |\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    # here we are checking the information present in the dataframe \n",
    "\n",
    "    iris_df.info()\n",
    "\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 150 entries, 0 to 149\n",
    "    Data columns (total 5 columns):\n",
    "     #   Column        Non-Null Count  Dtype  \n",
    "    ---  ------        --------------  -----  \n",
    "     0   sepal_length  150 non-null    float64\n",
    "     1   sepal_width   150 non-null    float64\n",
    "     2   petal_length  150 non-null    float64\n",
    "     3   petal_width   150 non-null    float64\n",
    "     4   species       150 non-null    object \n",
    "    dtypes: float64(4), object(1)\n",
    "    memory usage: 6.0+ KB\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    # the variable 'species' is a categorical datatype 'object'\n",
    "\n",
    "### The labelencoder() function is used to convert the categorical variable to its numeric counterpart. in this dataset 'species' is categorical. So we will encode this variable--:<a href=\"#The-labelencoder()-function-is-used-to-convert-the-categorical-variable-to-its-numeric-counterpart.-in-this-dataset-&#39;species&#39;-is-categorical.-So-we-will-encode-this-variable--:\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    # here we have created the variable called label encoder variable / object from the \n",
    "    # label encoder class\n",
    "    # so we have taken  label encoder from the preprocessing and store it in the label encoder \n",
    "    # here the object name is the label encoder \n",
    "\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    # here we are checking the unique values in the species column in the iris dataframe \n",
    "\n",
    "    iris_df['species'].unique()\n",
    "\n",
    "Out\\[9\\]:\n",
    "\n",
    "    array(['setosa', 'versicolor', 'virginica'], dtype=object)\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    # so we need to encode all these 3 categories of the species variable\n",
    "\n",
    "## The fit_transform() method calculates the mean and variance of each feature and transforms all the features using the respective mean and variance.<a href=\"#The-fit_transform()-method-calculates-the-mean-and-variance-of-each-feature-and-transforms-all-the-features-using-the-respective-mean-and-variance.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    # here we have created the variable iris_df['species']\n",
    "    # then we are fitting and transforming the species column of the iris dataframe through the \n",
    "    # label encoder \n",
    "    # we have used the label encoder to get the numerical value of the categorical values\n",
    "\n",
    "    iris_df['species'] = label_encoder.fit_transform(iris_df['species'])\n",
    "\n",
    "In \\[46\\]:\n",
    "\n",
    "    # here we have checked the bottom values of the iris dataframe \n",
    "\n",
    "    iris_df.tail()\n",
    "\n",
    "Out\\[46\\]:\n",
    "\n",
    "|     | sepal_length | sepal_width | petal_length | petal_width | species |\n",
    "|-----|--------------|-------------|--------------|-------------|---------|\n",
    "| 145 | 6.7          | 3.0         | 5.2          | 2.3         | 2       |\n",
    "| 146 | 6.3          | 2.5         | 5.0          | 1.9         | 2       |\n",
    "| 147 | 6.5          | 3.0         | 5.2          | 2.0         | 2       |\n",
    "| 148 | 6.2          | 3.4         | 5.4          | 2.3         | 2       |\n",
    "| 149 | 5.9          | 3.0         | 5.1          | 1.8         | 2       |\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    # here we have checked the information present in the iris dataframe \n",
    "\n",
    "    iris_df.info()\n",
    "\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 150 entries, 0 to 149\n",
    "    Data columns (total 5 columns):\n",
    "     #   Column        Non-Null Count  Dtype  \n",
    "    ---  ------        --------------  -----  \n",
    "     0   sepal_length  150 non-null    float64\n",
    "     1   sepal_width   150 non-null    float64\n",
    "     2   petal_length  150 non-null    float64\n",
    "     3   petal_width   150 non-null    float64\n",
    "     4   species       150 non-null    int32  \n",
    "    dtypes: float64(4), int32(1)\n",
    "    memory usage: 5.4 KB\n",
    "\n",
    "## As the dtype of species changes from object --> numeric we can conclude that with LabelEncoder transformation of a categorical variable to a numeric variable can be done<a href=\"#As-the-dtype-of-species-changes-from-object---%3E-numeric-we-can-conclude-that-with-LabelEncoder-transformation-of-a-categorical-variable-to-a-numeric-variable-can-be-done\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    # here we are importing the libraries for the ONE HOT ENCODING\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    # here we are importing the datasets function from the sklearn package \n",
    "    # and also importing the one hot encoder function from the sklearn preprocessing package \n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    # we need to load the iris dataset to create a dataframe iris_data \n",
    "    # y variable which will have our target values\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    # here we are loading the iris dataset  into the iris data \n",
    "\n",
    "    iris_data = datasets.load_iris()\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # what is the benifit of using the toy data \n",
    "    # you have the data dictionary inbuilt \n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    # here we are printing the keys present in the iris data \n",
    "\n",
    "    print(iris_data.keys())\n",
    "\n",
    "    dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "    # here we are checking the feature names present in the iris dataset \n",
    "    # here we will get all the independent variables as we are checking fpr the feature names\n",
    "    # and species is the dependent variable \n",
    "\n",
    "    iris_data.feature_names\n",
    "\n",
    "Out\\[15\\]:\n",
    "\n",
    "    ['sepal length (cm)',\n",
    "     'sepal width (cm)',\n",
    "     'petal length (cm)',\n",
    "     'petal width (cm)']\n",
    "\n",
    "In \\[16\\]:\n",
    "\n",
    "    # here wehave checked the target present in the iris dataset \n",
    "\n",
    "    iris_data.target\n",
    "\n",
    "Out\\[16\\]:\n",
    "\n",
    "    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
    "\n",
    "In \\[17\\]:\n",
    "\n",
    "    # here we are slicing the data and then we are compatinating the data \n",
    "    # we are having the iris data \n",
    "    # by using the pandas dataframe in the data and by using the numpy library we are creating the \n",
    "    # iris data which includes the data and target \n",
    "    # and we have created the columns which contains the feature names and target \n",
    "    # here we are combining because we are creating the data set \n",
    "    # then we have created the variable called y \n",
    "    #  and then we are pasisng the target values  through iris data and store it in the y \n",
    "\n",
    "    iris_data = pd.DataFrame(data=np.c_[iris_data[\"data\"],iris_data[\"target\"]],columns=\n",
    "                             iris_data[\"feature_names\"]+[\"target\"])\n",
    "    y = iris_data.target.values\n",
    "\n",
    "In \\[18\\]:\n",
    "\n",
    "    # c_is a function from numpy we use to slice the data and \n",
    "    # then concatenate displaying the data and columns in different axis\n",
    "\n",
    "In \\[19\\]:\n",
    "\n",
    "    # here we are checking the initial 2 rows of the iris dataset \n",
    "\n",
    "    iris_data.head(2)\n",
    "\n",
    "Out\\[19\\]:\n",
    "\n",
    "|     | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | target |\n",
    "|-----|-------------------|------------------|-------------------|------------------|--------|\n",
    "| 0   | 5.1               | 3.5              | 1.4               | 0.2              | 0.0    |\n",
    "| 1   | 4.9               | 3.0              | 1.4               | 0.2              | 0.0    |\n",
    "\n",
    "In \\[20\\]:\n",
    "\n",
    "    # here we are checking the shape of the iris dataset \n",
    "\n",
    "    iris_data.shape\n",
    "\n",
    "Out\\[20\\]:\n",
    "\n",
    "    (150, 5)\n",
    "\n",
    "## the onehotencoder has a feature called 'auto' which determines the categories automatically.<a href=\"#the-onehotencoder-has-a-feature-called-&#39;auto&#39;-which-determines-the-categories-automatically.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## reshape function to reshape the array(y) without modifying our data<a href=\"#reshape-function-to-reshape-the-array(y)-without-modifying-our-data\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## we also use an 'toarray() which helps us to represent our data in an array format<a href=\"#we-also-use-an-&#39;toarray()-which-helps-us-to-represent-our-data-in-an-array-format\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "    # here we are checking the y variable which is in the form of an array \n",
    "\n",
    "    y\n",
    "\n",
    "Out\\[21\\]:\n",
    "\n",
    "    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
    "           2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
    "           2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
    "           2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "    # here we have created the variable / object one hot encoder from the class one hot encoder \n",
    "    # and we have used (categories='auto') - it is used so that it can determines the category\n",
    "    # automatically\n",
    "    # now y is in the array format\n",
    "    # then we are reshaping the y and fitting and transforming it through the one hot encoder \n",
    "    # and store it into the y \n",
    "    # -1 means go for any dimenssion and 1 means changing the direction of an array \n",
    "    # then we will print the y in the form of an array \n",
    "\n",
    "    onehotencoder=OneHotEncoder(categories='auto')\n",
    "    y = onehotencoder.fit_transform(y.reshape(-1,1))\n",
    "    print(y.toarray())\n",
    "\n",
    "    [[1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [1. 0. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 1. 0.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]\n",
    "     [0. 0. 1.]]\n",
    "\n",
    "In \\[23\\]:\n",
    "\n",
    "    #  the reshaped array will have the resulting array in a single column that is 1D \n",
    "\n",
    "In \\[24\\]:\n",
    "\n",
    "    # lets find out the categories\n",
    "    # one hot encoding and creating dummies - there is only 1 difference \n",
    "    # when you are creating the dummies you are getting the dummies for  n1 categories \n",
    "    # and when you are doing the one hot encoding  then you are creating the numerical \n",
    "    # value for every categories \n",
    "    # ideally when we are creating the dummy we will get the  one less category \n",
    "    # \n",
    "    pd.get_dummies(iris_data.target).head()\n",
    "\n",
    "Out\\[24\\]:\n",
    "\n",
    "|     | 0.0 | 1.0 | 2.0 |\n",
    "|-----|-----|-----|-----|\n",
    "| 0   | 1   | 0   | 0   |\n",
    "| 1   | 1   | 0   | 0   |\n",
    "| 2   | 1   | 0   | 0   |\n",
    "| 3   | 1   | 0   | 0   |\n",
    "| 4   | 1   | 0   | 0   |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "     \n",
    "\n",
    "In \\[ \\]:"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
